{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "print(\"üîç Checking GPU availability...\")\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected. Training will be very slow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained LLM (falcon-7b-instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# First, check if GPU is available\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current GPU: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "# Define model name\n",
    "MODEL_NAME = \"tiiuae/falcon-7b-instruct\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Add padding token if it doesn't exist\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    \n",
    "# Define quantization config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model with optimized settings\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quant_config,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Model Loaded Successfully!\")\n",
    "\n",
    "# Resize token embeddings to account for added special tokens\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Healthcare Dataset (MedQA - USMLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the MedQA dataset\n",
    "dataset = load_dataset(\"GBaker/MedQA-USMLE-4-options\", split=\"train\")\n",
    "print(f\"Dataset size: {len(dataset)} examples\")\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define max sequence length - use a consistent value\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# Process examples one at a time to avoid batching issues\n",
    "def preprocess_function(example):\n",
    "    # Ensure inputs are strings\n",
    "    question = example[\"question\"]\n",
    "    if isinstance(question, list):\n",
    "        question = \" \".join(question)\n",
    "    \n",
    "    answer = example[\"answer\"]\n",
    "    if isinstance(answer, list):\n",
    "        answer = \" \".join(answer)\n",
    "    \n",
    "    # Create the full text\n",
    "    full_text = f\"Question: {question}\\nAnswer: {answer}\"\n",
    "    \n",
    "    # Tokenize with fixed length\n",
    "    encoded = tokenizer(\n",
    "        full_text,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=None  # Return Python lists\n",
    "    )\n",
    "    \n",
    "    # Return the encoded example\n",
    "    return encoded\n",
    "\n",
    "# Process the dataset\n",
    "processed_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=dataset.column_names,\n",
    "    desc=\"Processing dataset\"\n",
    ")\n",
    "\n",
    "# Add labels for causal language modeling (same as input_ids)\n",
    "def add_labels(example):\n",
    "    example[\"labels\"] = example[\"input_ids\"].copy()\n",
    "    return example\n",
    "\n",
    "tokenized_datasets = processed_dataset.map(add_labels, desc=\"Adding labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Use data collator for language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # We're doing causal language modeling, not masked language modeling\n",
    ")\n",
    "\n",
    "# Split the dataset\n",
    "train_test_split = tokenized_datasets.train_test_split(test_size=0.2)\n",
    "train_dataset = train_test_split[\"train\"].select(range(2000))\n",
    "eval_dataset = train_test_split[\"test\"].select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement LoRA for Efficient Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Define LoRA config\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query_key_value\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "\n",
    "# Define Training Arguments with label_names\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,  # Reduced batch size for stability\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=10,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,  # Log more frequently to see progress\n",
    "    fp16=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"loss\",\n",
    "    save_total_limit=2,\n",
    "    report_to=\"tensorboard\",  # Enable TensorBoard reporting\n",
    "    label_names=[\"labels\"],  # Explicitly specify label names\n",
    "    gradient_accumulation_steps=4,  # Accumulate gradients for effective larger batch size\n",
    ")\n",
    "\n",
    "# Early Stopping Callback\n",
    "early_stopping = EarlyStoppingCallback(early_stopping_patience=3)\n",
    "\n",
    "# Create Trainer with data collator\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,  # Use the data collator for proper batching\n",
    "    callbacks=[early_stopping],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Track training time\n",
    "start_time = time.time()\n",
    "\n",
    "# Train Model\n",
    "print(\"\\nüöÄ Starting Training...\")\n",
    "trainer.train()\n",
    "\n",
    "# End timer\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "\n",
    "# Print Training Summary\n",
    "print(f\"\\n‚úÖ Training Completed in {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# After training completes, save the model\n",
    "print(\"\\nüì¶ Saving the fine-tuned model...\")\n",
    "\n",
    "# Save the full model\n",
    "trainer.save_model(\"./final_model\")\n",
    "\n",
    "# Save PEFT adapter specifically (more efficient)\n",
    "model.save_pretrained(\"./peft_adapter\")\n",
    "\n",
    "# Save tokenizer\n",
    "tokenizer.save_pretrained(\"./final_model\")\n",
    "tokenizer.save_pretrained(\"./peft_adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
